{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks with Pytorch\n",
    "\n",
    "Original Paper: https://arxiv.org/abs/1706.02216    \n",
    "Original Code: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/reddit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "sys.path.append('./')\n",
    "from utils import *\n",
    "logger = make_logger(name='graphsage_logger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Dataset\n",
    "path = os.path.join(os.getcwd(), '..', 'data', 'Reddit')\n",
    "dataset = Reddit(path)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 확인\n",
    "# Nodes: 232965, Node Features: 602\n",
    "logger.info(f\"Node Feature Matrix Info: # Nodes: {data.x.shape[0]}, # Node Features: {data.x.shape[1]}\")\n",
    "\n",
    "# Edge Index\n",
    "# Graph Connectivity- COO (2, num_edges)형태의 그래프 연결 = (2, 114,615,892=1.14억)\n",
    "logger.info(f\"Edge Index Shape: {data.edge_index.shape}\")\n",
    "logger.info(f\"Edge Weight: {data.edge_attr}\")\n",
    "\n",
    "# train_mask는 훈련할 nodes을 나타낸다. (153431 nodes)\n",
    "print(data.train_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sampler\n",
    "train_loader = NeighborSampler(\n",
    "    data.edge_index, node_idx=data.train_mask,\n",
    "    sizes=[25, 10], batch_size=1024, shuffle=True, num_workers=12)\n",
    "\n",
    "subgraph_loader = NeighborSampler(\n",
    "    data.edge_index, node_idx=None,\n",
    "    sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeighborSampler Size\n",
    "sizes=[25, 10]는 Neighbor Sampling에서 사용할 k-hop 크기를 나타낸다.        \n",
    "sizes 리스트의 첫 번째 값인 25는 1-hop neighbor와 2-hop neighbor까지 사용하여 샘플링할 개수를 의미한다. 이는 중심 노드와 인접한 노드, 그리고 이들의 이웃 노드까지 모두를 포함하여 최대 25개까지 샘플링한다는 것을 뜻한다. 즉, 이 값은 한 번에 샘플링할 이웃 노드의 최대 수를 결정하는 것이다.    \n",
    "두 번째 sizes 값인 10은 샘플링할 이웃 노드의 개수를 의미한다. 이 값은 첫 번째 값과는 다르게 1-hop neighbor에 대해서만 샘플링을 하며, 2-hop neighbor는 사용하지 않는다. 따라서 이 값은 첫 번째 값보다 작다.    \n",
    "    \n",
    "예를 들어, 1-hop neighbor가 A, B, C, D, E, F인 노드 u에 대해 첫 번째 값 25와 두 번째 값 10을 가진 sizes를 사용하면 다음과 같은 샘플링이 이루어진다.    \n",
    "1) 1-hop neighbor에서 25개 노드를 샘플링한다. 예를 들어, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y가 선택됩니다.    \n",
    "     1-2) 2-hop neighbor에서 25개 노드를 샘플링합니다. 예를 들어, 2-hop neighbor가 a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y일 때, A와 이웃인 a, b, c, d, e와 B와 이웃인 f, g, h, i, j를 선택합니다.    \n",
    "2) 1-hop neighbor에서 10개 노드를 샘플링합니다. 예를 들어, A, B, C, D, E, F, G, H, I, J가 선택됩니다.\n",
    "즉, 첫 번째 값은 1-hop neighbor와 2-hop neighbor까지 모두 사용하여 샘플링하며, 두 번째 값은 1-hop neighbor에 대해서만 샘플링한다. 이는 모델의 복잡도를 조절할 수 있는 하이퍼파라미터이다.    \n",
    "이렇게 여러 개의 k-hop 크기를 사용하여 샘플링하는 것은 더 넓은 범위의 이웃 노드 정보를 포함할 수 있도록 하여 모델 성능을 높일 수 있다.        \n",
    "sizes는 하나의 리스트로 구성되어 있으며, 리스트 내의 값이 클수록 더 많은 이웃 노드 정보를 사용하여 샘플링한다.    \n",
    "\n",
    "### num_workers \n",
    "num_workers는 NeighborSampler가 데이터를 로딩할 때 사용할 worker(thread)의 수를 의미합니다. 즉, 병렬적인 데이터 로딩을 위한 파라미터입니다. 이 값을 높일수록 데이터 로딩 속도가 빨라지지만, 시스템 리소스를 많이 사용하게 되어 메모리나 CPU 사용량 등을 고려하여 적절한 값을 선택해야 합니다. 기본값은 1이며, 사용 가능한 하드웨어 리소스에 따라 값을 조정할 수 있습니다.\n",
    "\n",
    "### subgraph_loader와 train_loader\n",
    "train_loader는 훈련 데이터셋을 위한 Neighbor Sampling을 수행한다. 이 때 node_idx 인자에는 훈련 데이터셋에서 사용될 노드 인덱스를 전달한다.     \n",
    "반면에 subgraph_loader는 일반적으로 테스트 데이터셋에서 사용된다. node_idx 인자에는 None 값을 전달한다. sizes 인자는 -1로 설정하여 원래 그래프 전체를 사용하지 않고, 부분 그래프(subgraph)를 샘플링한다. 이렇게 샘플링된 부분 그래프는 전체 그래프의 근사값(approximation)으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, n_id, adjs = next(iter(train_loader))\n",
    "\n",
    "logger.info(f\"Current Batch Size: {batch_size}\")\n",
    "logger.info(f\"현재 Subgraph에서 사용된 모든 node id의 개수: {n_id.shape[0]}\")\n",
    "logger.info(f\"Layer의 수: {len(adjs)}\")\n",
    "\n",
    "A = adjs[1].size[0] - batch_size\n",
    "B = adjs[0].size[0] - A - batch_size\n",
    "\n",
    "logger.info(f\"진행 방향: {B}개의 2-hop neighbors ->\"\n",
    "            f\"{A}개의 1-hop neighbors -> {batch_size}개의 Head Nodes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. batch_size : 현재 batch size를 의미함 (integer)    \n",
    "next(iter(train_loader))를 호출하면, batch_size에 해당하는 개수의 샘플들을 반환한다. 이때 반환된 데이터는 이전에 NeighborSampler에서 지정한 sizes 파라미터에 따라 다양한 크기의 서브그래프(subgraph)를 형성한다. 따라서 n_id는 현재 배치에 있는 모든 노드 ID를 담고 있으며, adjs는 현재 배치에 대해 형성된 서브그래프의 리스트이다. 이 서브그래프들은 이후 모델 학습에서 사용된다.\n",
    "\n",
    "2. n_id : 이번 Subgraph에서 사용된 모든 node id.    \n",
    "batch_size개의 Row를 예측하기 위해서는 1차 이웃 node A개가 필요하고, 1차 이웃 node A개를 위해서는 2차 이웃 node B개가 필요.     \n",
    "n_id.shape = batch_size + A + B\n",
    "\n",
    "3. adjs : 아래와 같이 Layer의 수가 2개이면 adjs는 길이 2의 List가 된다.(subgraph의 리스트)    \n",
    "head node가 있고 1-hop neighbors와 2-hop neighbors가 있다고 할 때, adjs[0]은 1번째 레이어(1-hop 이웃)와 관련된 정보를 나타내며, head node와 1-hop 이웃들의 관계를 설명한다.     \n",
    "adjs[1]은 2번째 레이어(2-hop 이웃)와 관련된 정보를 나타내며, 1-hop 이웃들과 2-hop 이웃들 간의 관계를 설명한다. \n",
    "\n",
    "4. 각 리스트에는 (edge_index, e_id, size) 튜플이 들어있다.    \n",
    "edge_index: source -> target nodes를 기록한 bipartite edges    \n",
    "e_id: 위 edge_index에 들어있는 index가 Full Graph에서 갖는 node id    \n",
    "size: 위 edge_index에 들어있는 node의 수를 튜플로 나타낸 것으로 head -> 1-hop 관계를 예시로 들면, head node의 수가 a개, 1-hop node의 수가 b개라고 했을 때 size = (a+b, a). 또한 target node의 경우 source nodes의 리스트의 시작 부분에 포함되어 있어 skip-connections나 self-loops를 쉽게 사용할 수 있게 되어 있다.   \n",
    "   \n",
    "4-1. bipartite edges    \n",
    "Bipartite edge는 두 개의 독립된 노드 집합 간의 엣지를 의미한다. 예를 들어, 영화 추천 시스템에서 사용되는 경우, 한 집합은 사용자 노드이고 다른 집합은 영화 노드이다. 이 때, 사용자와 영화 간의 평점을 나타내는 엣지는 bipartite edge가 된다. 일반적으로, bipartite edge는 각 집합 내에서는 연결되어 있지 않으며 다른 집합 내의 노드와만 연결되어 있다.     \n",
    "    \n",
    "4-2. edge    \n",
    "edge란 그래프에서 노드들 간의 연결을 의미하는 선이다. edge는 노드들 간의 상호작용을 모델링하는 데 사용된다. 예를 들어, 소셜 네트워크에서 두 사람 간의 친구 관계, 도시 간의 도로, 단어 간의 유사도 등을 edge로 모델링할 수 있다. 따라서 edge는 그래프 신경망에서 정보 전달과 모델의 예측을 수행하는 중요한 역할을 한다.    \n",
    "    \n",
    "    \n",
    "5. edge_attr, edge_index?   \n",
    "edge_index는 각 엣지의 시작 노드와 끝 노드의 인덱스를 나타내는 텐서다. 예를 들어, (2, 4)가 edge_index에 있다면, 2번 노드와 4번 노드 사이에 엣지가 존재한다는 것을 의미한다. edge_attr은 엣지의 특성 값을 나타내는 텐서이며, 예를 들어 엣지의 가중치나 거리 등을 나타낼 수 있다. 따라서 edge_index와 edge_attr은 서로 다른 정보를 담고 있는 텐서다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes 는 항상 먼저 placed된다.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "\n",
    "            # 마지막 Layer는 Dropout을 적용하지 않는다.\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # # 모든(All) 사용 가능한 edges를 사용하여 노드 표현을 계층별로 계산한다. \n",
    "        # 이는 각 batch의 final representations을 즉시 계산하는 것과 대조적으로 \n",
    "        # 더 빠른 계산으로 이어진다.\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=int(data.train_mask.sum()))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` 는 `(edge_index, e_id, size)` tuples의 list를 가지고 있다.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[n_id], adjs)\n",
    "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / int(data.train_mask.sum())\n",
    "    return loss, approx_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model.inference(x)\n",
    "\n",
    "    y_true = y.cpu().unsqueeze(-1)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    results = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 3):\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
    "    # train_acc, val_acc, test_acc = test()\n",
    "    # print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31a978b9b36123e657517a00f2bc515935c5b3db5200b3f6166774c20a94c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
