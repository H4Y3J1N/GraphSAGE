{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks with Pytorch\n",
    "\n",
    "Original Paper: https://arxiv.org/abs/1706.02216    \n",
    "Original Code: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/reddit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "sys.path.append('./')\n",
    "from utils import *\n",
    "logger = make_logger(name='graphsage_logger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Dataset\n",
    "path = os.path.join(os.getcwd(), '..', 'data', 'Reddit')\n",
    "dataset = Reddit(path)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 확인\n",
    "# Nodes: 232965, Node Features: 602\n",
    "logger.info(f\"Node Feature Matrix Info: # Nodes: {data.x.shape[0]}, # Node Features: {data.x.shape[1]}\")\n",
    "\n",
    "# Edge Index\n",
    "# Graph Connectivity- COO (2, num_edges)형태의 그래프 연결 = (2, 114,615,892=1.14억)\n",
    "logger.info(f\"Edge Index Shape: {data.edge_index.shape}\")\n",
    "logger.info(f\"Edge Weight: {data.edge_attr}\")\n",
    "\n",
    "# train_mask는 훈련할 nodes을 나타낸다. (153431 nodes)\n",
    "print(data.train_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sampler\n",
    "train_loader = NeighborSampler(\n",
    "    data.edge_index, node_idx=data.train_mask,\n",
    "    sizes=[25, 10], batch_size=1024, shuffle=True, num_workers=12)\n",
    "\n",
    "subgraph_loader = NeighborSampler(\n",
    "    data.edge_index, node_idx=None,\n",
    "    sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look\n",
    "batch_size, n_id, adjs = next(iter(train_loader))\n",
    "\n",
    "# 1) batch_size\n",
    "# 현재 batch size를 의미함 (integer)\n",
    "logger.info(f\"Current Batch Size: {batch_size}\")\n",
    "\n",
    "# 2) n_id\n",
    "# 이번 Subgraph에서 사용된 모든 node id\n",
    "# batch_size개의 Row를 예측하기 위해서 이에 대한 1차 이웃 node A개가 필요하고\n",
    "# 1차 이웃 node A개를 위해서는 2차 이웃 node B개가 필요\n",
    "# n_id.shape = batch_size + A + B\n",
    "logger.info(f\"현재 Subgraph에서 사용된 모든 node id의 개수: {n_id.shape[0]}\")\n",
    "\n",
    "# 3) adjs\n",
    "# 아래와 같이 Layer의 수가 2개이면 adjs는 길이 2의 List가 된다.\n",
    "# head node가 있고 1-hop neighbors와 2-hop neighbors가 있다고 할 때\n",
    "# adjs[1]이 head node와 1-hop neighbors의 관계를 설명하며  (1번째 Layer)\n",
    "# adjs[0]이 1-hop neighbors와 2-hop neighbors의 관계를 설명한다. (2번째 Layer)\n",
    "logger.info(f\"Layer의 수: {len(adjs)}\")\n",
    "\n",
    "# 각 리스트에는 아래와 같은 튜플이 들어있다.\n",
    "# (edge_index, e_id, size)\n",
    "# edge_index: source -> target nodes를 기록한 bipartite edges\n",
    "# e_id: 위 edge_index에 들어있는 index가 Full Graph에서 갖는 node id\n",
    "\n",
    "# size: 위 edge_index에 들어있는 node의 수를 튜플로 나타낸 것으로\n",
    "# head -> 1-hop 관계를 예시로 들면,\n",
    "# head node의 수가 a개, 1-hop node의 수가 b개라고 했을 때\n",
    "# size = (a+b, a)\n",
    "# 또한 target node의 경우 source nodes의 리스트의 시작 부분에 포함되어 있어\n",
    "# skip-connections나 self-loops를 쉽게 사용할 수 있게 되어 있음\n",
    "A = adjs[1].size[0] - batch_size\n",
    "B = adjs[0].size[0] - A - batch_size\n",
    "\n",
    "logger.info(f\"진행 방향: {B}개의 2-hop neighbors ->\"\n",
    "            f\"{A}개의 1-hop neighbors -> {batch_size}개의 Head Nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes 는 항상 먼저 placed된다.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "\n",
    "            # 마지막 Layer는 Dropout을 적용하지 않는다.\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # # 모든(All) 사용 가능한 edges를 사용하여 노드 표현을 계층별로 계산한다. \n",
    "        # 이는 각 batch의 final representations을 즉시 계산하는 것과 대조적으로 \n",
    "        # 더 빠른 계산으로 이어진다.\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "        return x_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명 마크다운 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=int(data.train_mask.sum()))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` 는 `(edge_index, e_id, size)` tuples의 list를 가지고 있다.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[n_id], adjs)\n",
    "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / int(data.train_mask.sum())\n",
    "    return loss, approx_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model.inference(x)\n",
    "\n",
    "    y_true = y.cpu().unsqueeze(-1)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    results = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 3):\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
    "    # train_acc, val_acc, test_acc = test()\n",
    "    # print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31a978b9b36123e657517a00f2bc515935c5b3db5200b3f6166774c20a94c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
